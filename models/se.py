import torch
from torch.nn import Conv2d, SiLU, Sigmoid, AdaptiveAvgPool2d, Module


class SEModule(Module):
    def __init__(self, channels: int, reduction: int):
        """
        Механизм Squeeze-and-Excitation (SE) представляет собой модуль, для адаптивной перекалибровки каналов признаков. Его основная идея – динамически выделять 
        более информативные каналы и подавлять менее важные. Принцип работы SE-модуля:

        1. Squeeze (Сжатие)
        На этом этапе пространственная информация сводится к вектору, отражающему глобальные характеристики по каждому каналу. 
        Для этого применяется Global Average Pooling.  
        - Цель: Собрать статистическую сводку (агрегировать) по всему пространству для каждого канала.
        - Результат: Для входного тензора размером H x W x C получается вектор размерности 1 x 1 x C, где каждый элемент описывает важность соответствующего канала в целом изображении.

        2. Excitation (Возбуждение)
        После сжатия производится «возбуждение» – восстановление и калибровка информации для каждого канала.  
        - Процесс: Вектор из шага Squeeze пропускается через небольшую нейронную сеть, обычно состоящую из двух полносвязных слоев с нелинейностями.  
        - Первый слой: Уменьшает размерность (с помощью коэффициента редукции, например, reduction), что позволяет моделировать взаимосвязи между каналами.
        - Второй слой: Восстанавливает исходное число каналов.  
        - Функция активации: Часто используется ReLU после первого слоя и сигмоида после второго, что позволяет получить веса в диапазоне [0, 1] для каждого канала.
        - Результат: Получается вектор с весовыми коэффициентами для каждого канала, отражающий его относительную важность.

        3. Recalibration (Перекалибровка)
        Последним этапом является масштабирование исходных признаков с учетом вычисленных коэффициентов.  
        - Операция: Каждый канал входного тензора умножается на соответствующий скаляр из вектора возбуждения.  
        - Эффект: Это позволяет сети «перекалибровать» активацию каналов – усилить те, которые важны, и ослабить менее значимые, 
                  тем самым улучшая представление признаков для последующих слоев.

        Итоговая идея: Модуль SE позволяет сети динамически корректировать вклад каждого канала в зависимости от глобального контекста.
        """
        super(SEModule, self).__init__()
        self.avg_pool = AdaptiveAvgPool2d(1) # Приводим все feature maps к вектору размерности 

        # Здесь свертка выступает в качетсве линейного слоя, сжимающего вход. Так как изначально у нас были матрицы
        # feature maps размерности HxW, то после пулинга мы получим матрицы размерности 1x1. Чтобы не решейпить
        # тензоры, применяем такой подход.
        self.fc1 = Conv2d(channels, channels // reduction, kernel_size=1, padding=0)

        # Второй слой, восстанавливающий исходную размерность
        self.fc2 = Conv2d(channels // reduction, channels, kernel_size=1, padding=0)

        # Инициализируем слои
        torch.nn.init.xavier_uniform_(self.fc1.weight.data)
        torch.nn.init.xavier_uniform_(self.fc2.weight.data)

        # Активации
        self.relu = SiLU(inplace=True)
        self.sigmoid = Sigmoid()

    def forward(self, x):
        # Squeeze
        out = self.avg_pool(x)

        # Exication
        out = self.fc1(out)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.sigmoid(out)

        # Recolibration
        return out * x